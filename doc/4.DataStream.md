# 4.DataStream

**Flink 运行模型**

![图 Flink查询模型](https://github.com/yichao0803/myflink/raw/master/image/4StreamExecutionEnvironment-2.png)

以上为 Flink 的运行模型，Flink 的程序主要由三部分构成，分别为 Source、Transformation、Sink。DataSource 主要负责数据的读取，Transformation 主要负责对属于的转换操作，Sink 负责最终数据的输出。

**Flink 程序架构**

每个 Flink 程序都包含以下的若干流程：

- 获得一个执行环境；（Execution Environment）
- 加载/创建初始数据；（Source）
- 指定转换这些数据；（Transformation）
- 指定放置计算结果的位置；（Sink）
- 触发程序执行。

## 4.1 Setup Environment (Local and Remote)

执行环境 StreamExecutionEnvironment 是所有 Flink 程序的基础。

创建执行环境有三种方式，分别为：

```java
StreamExecutionEnvironment.getExecutionEnvironment 
StreamExecutionEnvironment.createLocalEnvironment 
StreamExecutionEnvironment.createRemoteEnvironment
```

#### 4.1.1 StreamExecutionEnvironment.getExecutionEnvironment

创建一个执行环境，表示当前执行程序的上下文。 如果程序是独立调用的，则此方法返回本地执行环境；如果从命令行客户端调用程序以提交到集群，则此方法返回此集群的执行环境，也就是说，getExecutionEnvironment 会根据查询运行的方式决定返回什么样的运行环境，是最常用的一种创建执行环境的方式。

```scala
val env = StreamExecutionEnvironment.getExecutionEnvironment
```

#### 4.1.2 StreamExecutionEnvironment.createLocalEnvironment

返回本地执行环境，需要在调用时指定默认的并行度。

```scala
val env = StreamExecutionEnvironment.createLocalEnvironment(1)
```

#### 4.1.3 StreamExecutionEnvironment.createRemoteEnvironment

返回集群执行环境，将Jar提交到远程服务器。需要在调用时指定JobManager的IP和端口号，并指定要在集群中运行的Jar包。

```scala
val env = StreamExecutionEnvironment.createRemoteEnvironment(1)
```



![StreamExecutionEnvironment](D:\git\yichao0803\myflink\image\4StreamExecutionEnvironment-01.png)


## 4.2 Source 
### 4.2.1 Built-in Source(readTextFile,fromCollection,etc)

源是程序读取其输入的位置。您可以使用将源附加到程序`StreamExecutionEnvironment.addSource(sourceFunction)`。Flink附带了许多预先实现的源函数，但是您始终可以通过实现`SourceFunction` for非并行源，实现`ParallelSourceFunction`接口或扩展 `RichParallelSourceFunction`for并行源来编写自己的自定义源。

可从以下位置访问几个预定义的流源`StreamExecutionEnvironment`：

基于文件：

- `readTextFile(path)`- `TextInputFormat`逐行读取文本文件，即符合规范的文件，并将其作为字符串返回。

- `readFile(fileInputFormat, path)` -根据指定的文件输入格式读取（一次）文件。

- `readFile(fileInputFormat, path, watchType, interval, pathFilter, typeInfo)`-这是前两个内部调用的方法。它`path`根据给定的读取文件`fileInputFormat`。根据提供的内容`watchType`，此源可以定期（每`interval`ms）监视路径中的新数据（`FileProcessingMode.PROCESS_CONTINUOUSLY`），或处理一次路径中当前的数据并退出（`FileProcessingMode.PROCESS_ONCE`）。使用`pathFilter`，用户可以进一步从文件中排除文件。

  *实施：*

  在后台，Flink将文件读取过程分为两个子任务，即*目录监视*和*数据读取*。这些子任务中的每一个都是由单独的实体实现的。监视由单个**非并行**（并行度= 1）任务实现，而读取由并行运行的多个任务执行。后者的并行性等于作业并行性。单个监视任务的作用是扫描目录（根据定期扫描或仅扫描一次`watchType`），找到要处理的文件，将它们*分成多个部分*，并将这些拆分分配给下游阅读器。读者将是阅读实际数据的人。每个拆分只能由一个阅读器读取，而阅读器可以一一阅读多个拆分。

  *重要笔记：*

  1. 如果将`watchType`设置为`FileProcessingMode.PROCESS_CONTINUOUSLY`，则在修改文件时，将完全重新处理其内容。这可能会破坏“完全一次”的语义，因为在文件末尾附加数据将导致重新处理其**所有**内容。
  2. 如果将`watchType`设置为`FileProcessingMode.PROCESS_ONCE`，则源将扫描路径**一次**并退出，而无需等待读取器完成文件内容的读取。当然，读者将继续阅读，直到读取了所有文件内容。关闭源将导致在该点之后没有更多检查点。这可能导致节点故障后恢复速度变慢，因为作业将从上一个检查点恢复读取。

基于套接字：

- `socketTextStream`-从套接字读取。元素可以由定界符分隔。

基于集合：

- `fromCollection(Collection)`-从Java Java.util.Collection创建数据流。集合中的所有元素必须具有相同的类型。
- `fromCollection(Iterator, Class)`-从迭代器创建数据流。该类指定迭代器返回的元素的数据类型。
- `fromElements(T ...)`-从给定的对象序列创建数据流。所有对象必须具有相同的类型。
- `fromParallelCollection(SplittableIterator, Class)`-从迭代器并行创建数据流。该类指定迭代器返回的元素的数据类型。
- `generateSequence(from, to)` -并行生成给定间隔中的数字序列。

Custom Source：

- `addSource`-附加新的源功能。例如，要阅读Apache Kafka，可以使用 `addSource(new FlinkKafkaConsumer010<>(...))`。有关更多详细信息，请参见[连接器](https://ci.apache.org/projects/flink/flink-docs-master/dev/connectors/index.html)。

### 4.2.2 Custom Source (addSource)

- `addSource` - Attach a new source function. For example, to read from Apache Kafka you can use `addSource(new FlinkKafkaConsumer08<>(...))`. See [connectors](https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/connectors/index.html) for more details.

## 4.3 Sink 
  * Built-in Sink(writeAsText,writeAsCsv,etc)

    数据接收器使用DataStream，并将其转发到文件，套接字，外部系统或打印它们。Flink带有各种内置的输出格式，这些格式封装在DataStream的操作后面：

    - `writeAsText()`/ `TextOutputFormat`-将元素按行写为字符串。通过调用每个元素的*toString（）*方法获得字符串。
    - `writeAsCsv(...)`/ `CsvOutputFormat`-将元组写为逗号分隔的值文件。行和字段定界符是可配置的。每个字段的值来自对象的*toString（）*方法。
    - `print()`/ `printToErr()` - 在标准输出/标准错误流上打印每个元素的*toString（）*值。可选地，可以提供前缀（msg），该前缀在输出之前。这可以帮助区分不同的*打印*调用。如果并行度大于1，则输出之前还将带有产生输出的任务的标识符。
    - `writeUsingOutputFormat()`/ `FileOutputFormat`-自定义文件输出的方法和基类。支持自定义对象到字节的转换。
    - `writeToSocket` -根据以下内容将元素写入套接字 `SerializationSchema`
    - `addSink`-调用自定义接收器功能。Flink捆绑有连接到其他系统（例如Apache Kafka）的连接器，这些连接器实现为接收器功能。

    请注意，上的`write*()`方法`DataStream`主要用于调试目的。它们不参与Flink的检查点，这意味着这些功能通常具有至少一次的语义。刷新到目标系统的数据取决于OutputFormat的实现。这意味着并非所有发送到OutputFormat的元素都立即显示在目标系统中。同样，在失败的情况下，这些记录可能会丢失。

    为了将流可靠，准确地一次传输到文件系统中，请使用`flink-connector-filesystem`。同样，通过该`.addSink(...)`方法的自定义实现可以参与Flink一次精确语义的检查点。

  * Custom Source(addSink)
## 4.4 Transformations
  * Basic Transformations
    * Map [DataStream->DataStream]：一对一转换,即一条转换成另一条。
    * Filter [DataStream->DataStream]：过滤出需要的数据
    * FlatMap [DataStream->DataStream]：一行变零到多行。
  * KeyedStream Transformations
    * KeyBy [DataStream->KeyedStream] ：按指定的 Key 对数据重分区。将同一 Key 的数据放到同一个分区。在内部 *keyBy（）* 是通过哈希分区实现的
    
      注意:
    
      * 分区结果和 KeyBy 下游算子的并行度强相关。如下游算子只有一个并行度,不管怎么分，都会分到一起。
      * 对于 POJO 类型，KeyBy 可以通过 keyBy(fieldName) 指定字段进行分区。
      * 对于 Tuple 类型，KeyBy可以通过 keyBy(fieldPosition) 指定字段进行分区。
      * 对于一般类型，如上, KeyBy可 以通过 keyBy(new KeySelector {...}) 指定字段进行分区。
    
    * Aggregations  [KeyedStream->DataStream]：Aggregate 对 KeyedStream 按指定字段滚动聚合并输出每一次滚动聚合后的结果。默认的聚合函数有:sum、min、minBy、max、mabBy。
    
      注意:
    
      * max(field) 与 maxBy(field) 的区别: maxBy 可以返回 field 最大的那条数据;而 max 则是将最大的 field 的值赋值给第一条数据并返回第一条数据。同理, min与 minBy。
      * Aggregate 聚合算子会滚动输出每一次聚合后的结果。
    
    * Reduce [KeyedStream->DataStream]：基于 ReduceFunction 进行滚动聚合，并向下游算子输出每次滚动聚合后的结果。
  * Multistream Transformations
    * Union
    
      在`DataStream`上使用`union`算子可以合并多个同类型的数据流，并生成同类型的数据流，即可以将多个`DataStream[T]`合并为一个新的`DataStream[T]`。数据将按照先进先出（First In First Out）的模式合并，且不去重。下图`union`对白色和深色两个数据流进行合并，生成一个数据流。
    
      ![hywpvume62](D:\git\yichao0803\myflink\image\4StreamExecutionEnvironment-3.png)
    
    * Connect, coMap, coFlatMap
    
      `union`虽然可以合并多个数据流，但有一个限制，即多个数据流的数据类型必须相同。`connect`提供了和`union`类似的功能，用来连接两个数据流，它与`union`的区别在于：
    
      1. `connect`只能连接两个数据流，`union`可以连接多个数据流。
      2. `connect`所连接的两个数据流的数据类型可以不一致，`union`所连接的两个数据流的数据类型必须一致。
      3. 两个`DataStream`经过`connect`之后被转化为`ConnectedStreams`，`ConnectedStreams`会对两个流的数据应用不同的处理方法，且双流之间可以共享状态。
    
      `connect`经常被应用在对一个数据流使用另外一个流进行控制处理的场景上，如下图所示。控制流可以是阈值、规则、机器学习模型或其他参数。
    
      ![j1ijb8m8ds](D:\git\yichao0803\myflink\image\4StreamExecutionEnvironment-4.png)
    
    * Split & select
  * Distribution Transformations 
    * Random
    * Round-Robin
    * Rescale
    * Broadcast
    * Global 
    * Custom
## 4.5 Data Types

### 4.5.1 Flink 中的类型处理

Flink 会尽力推断有关数据类型的大量信息，这些数据会在分布式计算期间被网络交换或存储。 可以把它想象成一个推断表结构的数据库。在大多数情况下，Flink 可以依赖自身透明的推断出所有需要的类型信息。 掌握这些类型信息可以帮助 Flink 实现很多意想不到的特性：

- 对于使用 POJOs 类型的数据，可以通过指定字段名（比如 `dataSet.keyBy("username")` ）进行 grouping 、joining、aggregating 操作。 类型信息可以帮助 Flink 在运行前做一些拼写错误以及类型兼容方面的检查，而不是等到运行时才暴露这些问题。
- Flink 对数据类型了解的越多，序列化和数据布局方案就越好。 这对 Flink 中的内存使用范式尤为重要（可以尽可能处理堆上或者堆外的序列化数据并且使序列化操作很廉价）。
- 最后，它还使用户在大多数情况下免于担心序列化框架以及类型注册。

通常在应用*运行之前的阶段 (pre-flight phase)*，需要数据的类型信息 - 也就是在程序对 `DataStream` 或者 `DataSet` 的操作调用之后，在 `execute()`、`print()`、`count()`、`collect()` 调用之前。

### 4.5.2  Flink 的 TypeInformation 类

  * 类 [TypeInformation](https://github.com/apache/flink/blob/master//flink-core/src/main/java/org/apache/flink/api/common/typeinfo/TypeInformation.java) 是所有类型描述符的基类。该类表示类型的基本属性，并且可以生成序列化器，在一些特殊情况下可以生成类型的比较器。 (*请注意，Flink 中的比较器不仅仅是定义顺序 - 它们是处理键的基础工具*)

    Flink 内部对类型做了如下区分：

    - 基础类型：所有的 Java 主类型（primitive）以及他们的包装类，再加上 `void`、`String`、`Date`、`BigDecimal` 以及 `BigInteger`。
    - 主类型数组（primitive array）以及对象数组
    - 复合类型
      - Flink 中的 Java 元组 (Tuples) (元组是 Flink Java API 的一部分)：最多支持25个字段，null 是不支持的。
      - Scala 中的 *case classes* (包括 Scala 元组)：null 是不支持的。
      - Row：具有任意数量字段的元组并且支持 null 字段。。
      - POJOs: 遵循某种类似 bean 模式的类。
    - 辅助类型 (Option、Either、Lists、Maps 等)
    - 泛型类型：这些不是由 Flink 本身序列化的，而是由 Kryo 序列化的。

    POJOs 是特别有趣的，因为他们支持复杂类型的创建以及在键的定义中直接使用字段名： `dataSet.join(another).where("name").equalTo("personName")` 它们对运行时也是透明的，并且可以由 Flink 非常高效地处理。

    #### POJO 类型的规则

    如果满足以下条件，Flink 会将数据类型识别为 POJO 类型（并允许“按名称”引用字段）：

    - 该类是公有的 (public) 和独立的（没有非静态内部类）
    - 该类拥有公有的无参构造器
    - 类（以及所有超类）中的所有非静态、非 transient 字段都是公有的（非 final 的）， 或者具有遵循 Java bean 对于 getter 和 setter 命名规则的公有 getter 和 setter 方法。

    请注意，当用户自定义的数据类型无法识别为 POJO 类型时，必须将其作为泛型类型处理并使用 Kryo 进行序列化。

    #### 创建 TypeInformation 或者 TypeSerializer

    要为类型创建 TypeInformation 对象，需要使用特定于语言的方法：

    因为 Java 会擦除泛型类型信息，所以需要将类型传入 TypeInformation 构造函数：

    对于非泛型类型，可以传入类型的 Class 对象：

    ```
    TypeInformation<String> info = TypeInformation.of(String.class);
    ```

    对于泛型类型，你需要通过 `TypeHint` 来“捕获”泛型类型信息：

    ```
    TypeInformation<Tuple2<String, Double>> info = TypeInformation.of(new TypeHint<Tuple2<String, Double>>(){});
    ```

    在内部，这会创建 TypeHint 的匿名子类，捕获泛型信息并会将其保留到运行时。

    通过调用 `TypeInformation` 对象的 `typeInfo.createSerializer(config)` 方法可以简单的创建 `TypeSerializer` 对象。

    `config` 参数的类型是 `ExecutionConfig`，这个参数中持有程序注册的自定义序列化器信息。 尽可能传入程序合适的 ExecutionConfig 。可以通过调用 `DataStream` 或者 `DataSet` 的 `getExecutionConfig()` 函数获得 ExecutionConfig 对象。如果是在一个函数的内部 （比如 `MapFunction`），可以使这个函数首先成为 [RichFunction](https://github.com/apache/flink/blob/master//flink-core/src/main/java/org/apache/flink/api/common/functions/RichFunction.java) ，然后通过调用 `getRuntimeContext().getExecutionConfig()` 获得 ExecutionConfig 对象。
## 4.6 Functions
## 4.7 Iterations

迭代流程序实现了一个步进函数，并将它嵌入到一个`IterativeStream`。由于DataStream程序可能永远不会完成，因此没有最大迭代次数。相反，您需要使用`split`转换或，指定流的哪一部分反馈给迭代，哪一部分向下游转发`filter`。在这里，我们展示了一个使用过滤器的示例。首先，我们定义一个`IterativeStream`

```
IterativeStream<Integer> iteration = input.iterate();
```

然后，我们使用一系列转换（此处为简单`map`转换）指定将在循环内执行的逻辑

```
DataStream<Integer> iterationBody = iteration.map(/* this is executed many times */);
```

要关闭迭代并定义迭代尾部，请调用的`closeWith(feedbackStream)`方法`IterativeStream`。提供给该`closeWith`函数的DataStream 将反馈到迭代头。一种常见的模式是使用过滤器将反馈的部分流和向前传播的部分分开。这些过滤器可以，例如，定义“终止”逻辑，其中元素被允许向下游传播而不是被反馈。

```
iteration.closeWith(iterationBody.filter(/* one part of the stream */));
DataStream<Integer> output = iterationBody.filter(/* some other part of the stream */);
```

例如，以下程序从一系列整数中连续减去1，直到它们达到零为止：

```
DataStream<Long> someIntegers = env.generateSequence(0, 1000);

IterativeStream<Long> iteration = someIntegers.iterate();

DataStream<Long> minusOne = iteration.map(new MapFunction<Long, Long>() {
  @Override
  public Long map(Long value) throws Exception {
    return value - 1 ;
  }
});

DataStream<Long> stillGreaterThanZero = minusOne.filter(new FilterFunction<Long>() {
  @Override
  public boolean filter(Long value) throws Exception {
    return (value > 0);
  }
});

iteration.closeWith(stillGreaterThanZero);

DataStream<Long> lessThanZero = minusOne.filter(new FilterFunction<Long>() {
  @Override
  public boolean filter(Long value) throws Exception {
    return (value <= 0);
  }
});
```



## 4.8 Time-Based and Window Operators
  * Time characteristics
    * Processing Time 
    * Event Time 
    * Ingestion Time
  * Assign Timestamps and generating watemarks
  * ProcessFunction 
    * TimeService and Timer
    * Emitting to Side Outputs（发射至侧面输出）
  * Windows
    * Window Assigners (窗口分配器)
    * Windos Functions 
    * Triggers (触发器)
    * Evictors (驱逐者)
    * Allowed Lateness （允许延迟）
    * Side Output (侧输出)
  * Joining
    * Windows Join 
      * Tumbling Window Join 
      * Sliding Window Join 
      * Session Window Join 
    * Interval Join 
  * 
## 4.9 Async I/O

## 4.10 参考资料

* [operators](https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/stream/operators/)
* [iterations](https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/datastream_api.html#iterations)

* [数据类型以及序列化](https://ci.apache.org/projects/flink/flink-docs-release-1.10/zh/dev/types_serialization.html)
* [Flink DataStream 算子 Map、FlatMap、Filter、KeyBy、Reduce、Fold、Aggregate](https://blog.csdn.net/wangpei1949/article/details/101625394)
* [Flink算子使用方法及实例演示：union和connect](https://cloud.tencent.com/developer/article/1560680)

* [Flink 原理与实现：数据流上的类型和操作](http://wuchong.me/blog/2016/05/20/flink-internals-streams-and-operations-on-streams/)
